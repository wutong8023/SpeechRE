# @package _global_

common:
  seed: 1

checkpoint:
  keep_last_epochs: 10
  patience: 10

task:
  _name: speech_to_text
  data: ???
  data_config_yaml: config_st.yaml

dataset:
  train_subset: train_el-en_st,train_es-en_st,train_es-fr_st,train_es-it_st,train_es-pt_st,train_fr-en_st,train_fr-es_st,train_fr-pt_st,train_it-en_st,train_it-es_st,train_pt-en_st,train_pt-es_st,train_ru-en_st
  valid_subset: valid_el-en_st,valid_es-en_st,valid_es-fr_st,valid_es-it_st,valid_es-pt_st,valid_fr-en_st,valid_fr-es_st,valid_fr-pt_st,valid_it-en_st,valid_it-es_st,valid_pt-en_st,valid_pt-es_st,valid_ru-en_st
  num_workers: 4
  max_tokens: 40000
  skip_invalid_size_inputs_valid_test: True

criterion:
  _name: label_smoothed_cross_entropy
  report_accuracy: True
  label_smoothing: 0.1
  ignore_prefix_size: 1

optimization:
  lr: [2e-3]
  update_freq: [8]
  clip_norm: 10.0
  max_epoch: 200

optimizer:
  _name: adam

lr_scheduler:
  _name: inverse_sqrt
  warmup_updates: 10000

model:
  _name: s2t_transformer  # s2t_transformer_s

  encoder_embed_dim: 256
  encoder_ffn_embed_dim: 2048  # 256 * 8
  encoder_attention_heads: 4

  decoder_embed_dim: 256
  decoder_ffn_embed_dim: 2048  # 256 * 8
  decoder_attention_heads: 4
  decoder_output_dim: 256
  decoder_input_dim: 256

  dropout: 0.3
  attention_dropout: 0.3
  activation_dropout: 0.3